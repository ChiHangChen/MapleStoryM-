{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "\n",
    "target_size = (1280,720)\n",
    "model_input_shape = (299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_resize(path,target_size):\n",
    "    return cv2.resize(cv2.imread(path),target_size)/255\n",
    "\n",
    "def crop_img(inputImg, scenario,model_input_shape):\n",
    "    if scenario == 'reward':\n",
    "        return inputImg[:,335:940]\n",
    "    elif scenario == 'dialog':\n",
    "        return inputImg[460:,]\n",
    "    elif scenario == 'autoIcon':\n",
    "        icon = inputImg[632:698,351:416] # (66,65,3)\n",
    "        icon = cv2.resize(icon,model_input_shape)\n",
    "        return icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "def create_resnet_model(n_classes):\n",
    "    RESmodel=InceptionResNetV2(include_top=True, weights='imagenet')\n",
    "    RESmodel.layers.pop()\n",
    "    new_layer = Dense(n_classes, activation='softmax', name='my_dense')\n",
    "    inp = RESmodel.input\n",
    "    out = new_layer(RESmodel.layers[-1].output)\n",
    "    model = Model(inp, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train util get a good model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list = glob.glob('./scenario/**/*.*',recursive=True)\n",
    "print(f\"Data Size : {len(img_path_list)}\")\n",
    "img_cls = [os.path.basename(os.path.dirname(i)) for i in img_path_list]\n",
    "img_cls = [i if i not in ['idle','inQuest'] else 'others' for i in img_cls]\n",
    "print(f\"Data Size : {len(img_cls)}\")\n",
    "classes = list(np.unique(img_cls))\n",
    "print(classes)\n",
    "Y = np.zeros((len(img_cls),len(classes)))\n",
    "for r in range(len(img_cls)):\n",
    "    Y[r,classes.index(img_cls[r])] = 1\n",
    "    \n",
    "X_img = []\n",
    "for i in img_path_list:\n",
    "    img = read_and_resize(i,target_size)\n",
    "    X_img.append(cv2.resize(img,model_input_shape))\n",
    "X_img = np.array(X_img)\n",
    "print(X_img.shape)\n",
    "class_weight = compute_class_weight('balanced',np.unique(img_cls),img_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "count = 0\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    count+=1\n",
    "    print(f\"Training count : {count}\")   \n",
    "    model = create_resnet_model(len(classes))\n",
    "    model.compile(optimizer=Adam(lr=0.005), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='loss', min_delta=0.001, patience=10, verbose=1)\n",
    "    model.fit(X_img, Y,batch_size=6,epochs=200,callbacks=[reduce_lr, early_stopping],class_weight=class_weight)\n",
    "    score, accuracy = model.evaluate(X_img, Y,batch_size=6)\n",
    "    print(f\"Accuracy : {round(accuracy,3)}\")\n",
    "    if accuracy >= 0.99:\n",
    "        model.save(\"AutoQuest_first_model.h5\")\n",
    "        del model, accuracy\n",
    "        gc.collect()\n",
    "        break\n",
    "    else:\n",
    "        del model, accuracy\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list = glob.glob('./scenario/**/*.*',recursive=True)\n",
    "np.random.shuffle(img_path_list)\n",
    "img_cls = [os.path.basename(os.path.dirname(i)) for i in img_path_list]\n",
    "img_path_list = [i for j,i in enumerate(img_path_list) if img_cls[j] in ['idle','inQuest']]\n",
    "img_cls = [i for i in img_cls if i in ['idle','inQuest']]\n",
    "print(f\"Data Size : {len(img_cls)}\")\n",
    "classes = list(np.unique(img_cls))\n",
    "print(classes)\n",
    "Y = np.zeros((len(img_cls),len(classes)))\n",
    "for r in range(len(img_cls)):\n",
    "    Y[r,classes.index(img_cls[r])] = 1\n",
    "    \n",
    "X_img = []\n",
    "for i in img_path_list:\n",
    "    img = read_and_resize(i,target_size)\n",
    "    X_img.append(crop_img(img,'autoIcon',model_input_shape))\n",
    "X_img = np.array(X_img)\n",
    "print(X_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "count = 0\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    count+=1\n",
    "    print(f\"Training count : {count}\")\n",
    "    model = create_resnet_model(len(classes))\n",
    "    model.compile(optimizer=Adam(lr=0.005), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='loss', min_delta=0.001, patience=10, verbose=1)\n",
    "    model.fit(X_img, Y,batch_size=6,epochs=200,callbacks=[reduce_lr, early_stopping],class_weight=class_weight)\n",
    "    score, accuracy = model.evaluate(X_img, Y,batch_size=6)\n",
    "    print(f\"Accuracy : {round(accuracy,3)}\")\n",
    "    if accuracy >= 0.99:\n",
    "        model.save(\"AutoQuest_second_model.h5\")\n",
    "        del model, accuracy\n",
    "        gc.collect()\n",
    "        break\n",
    "    else:\n",
    "        del model, accuracy\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_classes = ['dialog', 'loading', 'others', 'reward']\n",
    "second_classes = ['idle', 'inQuest']\n",
    "def check_scenario(inputImg):\n",
    "    inputImg_1 = cv2.resize(inputImg,model_input_shape)\n",
    "    inputImg_1 = np.expand_dims(inputImg_1,0)\n",
    "    pred = model1.predict(inputImg_1)\n",
    "    first_scenario = first_classes[np.argmax(pred)]\n",
    "    if first_scenario != 'others':\n",
    "        return first_scenario\n",
    "    else:\n",
    "        inputImg_2 = crop_img(inputImg,'autoIcon',model_input_shape)\n",
    "        inputImg_2 = np.expand_dims(inputImg_2,0)\n",
    "        pred = model2.predict(inputImg_2)\n",
    "        second_scenario = second_classes[np.argmax(pred)]\n",
    "        return second_scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model(\"AutoQuest_first_model.h5\")\n",
    "model2 = load_model(\"AutoQuest_second_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list = glob.glob('./scenario/**/*.*',recursive=True)\n",
    "print(f\"Data Size : {len(img_path_list)}\")\n",
    "img_cls = [os.path.basename(os.path.dirname(i)) for i in img_path_list]\n",
    "    \n",
    "X_img = []\n",
    "for i in img_path_list:\n",
    "    img = read_and_resize(i,target_size)\n",
    "    X_img.append(img)\n",
    "X_img = np.array(X_img)\n",
    "output_scenario = [check_scenario(i) for i in X_img]\n",
    "print(confusion_matrix(img_cls,output_scenario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
